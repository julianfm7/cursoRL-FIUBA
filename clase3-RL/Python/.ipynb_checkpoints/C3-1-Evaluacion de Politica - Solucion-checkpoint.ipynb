{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# en caso de correrlo en google colab\n",
    "# de esta manera podremos tener la carpeta lib (donde se encuentra en ambiente Gridworld)\n",
    "\n",
    "import sys\n",
    "#if \"../\" not in sys.path:\n",
    "#  sys.path.append(\"../\") \n",
    "\n",
    "!git clone https://github.com/julianfm7/cursoRL-FIUBA\n",
    "\n",
    "# necesario en google colab para que sys.path busque\n",
    "# y encuentre la carpeta lib donde se encuentra el ambiente Gridworld\n",
    "\n",
    "!mv cursoRL-FIUBA cursoRLFIUBA\n",
    "\n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid World es el ambiente del libro de Sutton del capítulo 4. Un agente está en una grilla de MxN y el objetivo es llegar al estado terminal esquina superior izquierda o esquina inferior derecha.\n",
    "\n",
    "Por ejemplo, una grilla de 4x4 se ve así:\n",
    "\n",
    "T  o  o  o <br>\n",
    "o  o  o  o <br>\n",
    "o  x  o  o <br>\n",
    "o  o  o  T\n",
    "\n",
    "x es la posición del agente. T son los estados terminales.\n",
    "\n",
    "El agente puede ir hacia arriba(0), la derecha(1), abajo(2), izquierda(3). Si se choca con las paredes se queda estático. Cada movimiento 'cuesta' una unidad de reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  T\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env._render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  x  o  T\n"
     ]
    }
   ],
   "source": [
    "env.step(2)\n",
    "env._render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este ejercicio es evaluar la política aleatoria (que se mueve en las cuatro direcciones con la misma probabilidad).\n",
    "\n",
    "Recordar las ecuaciones y el algoritmo (de Sutton capítulo 4):\n",
    "\n",
    "<img src=\"ecuacion 4.5.PNG\">\n",
    "<img src=\"algoritmo de evaluacion.PNG\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluar una política dado un ambiente y una descripción completa\n",
    "    de la dinámica del ambiente.\n",
    "    \n",
    "    Argumentos:\n",
    "        política: matriz de tamaño [S, A] representando la política.\n",
    "        env: ambiente de OpenAI representadno las probabilidades de transición\n",
    "        del ambiente. \n",
    "        env.P[s][a] es una lista de tuplas (probabilidad, próximo_estado, recompensa, done) dado que estoy en s y tomo a\n",
    "        env.nS es el número de estados en el ambiente\n",
    "        env.nA es el número de acciones en el ambiente\n",
    "        theta: para la evaluación de la política una vez que la función de valor cambia menos que\n",
    "        theta para todos los estados\n",
    "        discount_factor: factor de descuento gama.\n",
    "        \n",
    "    Retorna:\n",
    "        Vector de longitud env.nS que representa la función de valor.\n",
    "    \"\"\"\n",
    "    # Empezar con función de valor nula\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    while True:\n",
    "        # TODO: Implementar!\n",
    "        #TIP: enumerate(lista) permite iterar sobre indice, elemento\n",
    "        delta = 0\n",
    "        for estado in range(env.nS):\n",
    "            v = 0\n",
    "            for accion in range(env.nA):\n",
    "                for tupla in env.P[estado][accion]:\n",
    "                    v = v + policy[estado][accion]*tupla[0]*(tupla[2] + discount_factor*V[tupla[1]])\n",
    "            delta = max(abs(V[estado] - v),delta)\n",
    "            #print(env.P)\n",
    "            V[estado] = v\n",
    "            #print(V)\n",
    "        if delta < theta:\n",
    "            break\n",
    "        # por cada estado en el env [0,1,...,nS-1]:\n",
    "          # inicializar en 0 la funcion valor para ese estado\n",
    "          # por cada accion posible:\n",
    "            # por cada posible transicion dado ese estado-accion:\n",
    "              # usar la formula para sumar el termino a la funcion valor del estado\n",
    "          \n",
    "          # usar una variable para guardar el cambio maximo de nueva funcion valor vs anterior funcion valor\n",
    "          # guardar funcion valor para el estado\n",
    "            \n",
    "        # si el cambio maximo en el update de la funcion valor para todos los estados es menor a theta, parar\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que la evaluación de la política funcionó como esperábamos\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "# si el próximo assert no genera ningún error entonces la evaluación de la política fue correcta\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos definir otra política y calcular la función de valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "otra_politica = np.array([1.0,0,0,0.0]*16)\n",
    "otra_politica = np.reshape(otra_politica,[16,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(otra_politica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta política es determinística. El agente siempre se mueve para arriba en todas las circunstancias. Si el factor de descuento es 1.0 el algoritmo no converge, y por eso pones 0.5 como factor de descuento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -1.99999237 -1.99999237 -1.99999237 -1.         -1.99999619\n",
      " -1.99999619 -1.99999619 -1.5        -1.99999809 -1.99999809 -1.99999809\n",
      " -1.75       -1.99999905 -1.99999905  0.        ]\n"
     ]
    }
   ],
   "source": [
    "v_otra = policy_eval(otra_politica,env,discount_factor=0.5)\n",
    "print(v_otra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensar en cómo se interpretan estos valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos definir otra política no determinística. En este caso el agente se mueve en todas las situaciones para arriba con probabilidad 1/8 y para cualquiera las otras direcciones con probabilidad 7/24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "otra_politica = np.array([0.125,0.2916666666666667,0.2916666666666667,0.2916666666666667]*16)\n",
    "otra_politica = np.reshape(otra_politica,[16,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667],\n",
       "       [0.125     , 0.29166667, 0.29166667, 0.29166667]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otra_politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.         -13.80749015 -18.45834526 -19.47950473 -18.67870582\n",
      " -19.53557013 -18.65948559 -17.07210798 -22.39845487 -20.29485346\n",
      " -16.02842138 -11.02442646 -22.66769046 -19.35453299 -12.20982148\n",
      "   0.        ]\n"
     ]
    }
   ],
   "source": [
    "v_otra = policy_eval(otra_politica,env,discount_factor=1.0)\n",
    "print(v_otra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
